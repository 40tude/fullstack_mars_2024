{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4N9XyPIoWHV"
      },
      "source": [
        "# <span style=\"color:orange\"><b>Run on PC</b></span>\n",
        "# Code Attention\n",
        "\n",
        "Show how to code an encoder decoder model with attention mechanism.\n",
        "Use generated data (the same generated data we used to demonstrate the encoder decoder project) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CqUIHNG_w-j"
      },
      "source": [
        "## Prelude\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyeOCpH_yRRe"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "import os\n",
        "import time\n",
        "\n",
        "from random import randint\n",
        "from numpy import array\n",
        "\n",
        "\n",
        "k_Vocab_Size_In   = 100\n",
        "k_In_Seq_Length   = 10\n",
        "k_Out_Seq_Length  = 5\n",
        "k_Batch_Size      = 128\n",
        "\n",
        "k_N_Word_Embed    = 32 # number of units needed for the embedding layers\n",
        "k_N_GRU           = 32 # number of units needed for the GRU \n",
        "k_N_W1_W2         =  8 # number of neurons in W1 and W2\n",
        "\n",
        "k_Train_Samples   = 10_000 \n",
        "k_Val_Samples     =  5_000 \n",
        "\n",
        "k_Epochs          = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yZhjrVO_6aw"
      },
      "source": [
        "## Generate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HQ-jiYSAD0D"
      },
      "outputs": [],
      "source": [
        "# generate a sequence of n random integers from 2 to val_max-1 included\n",
        "# TODO : pourquoi (2, val_max-1) et pas (1, val_max)\n",
        "# Qu'est ce qui empêche d'avoir 1 et val_max ?\n",
        "        \n",
        "def generate_sequence(n, val_max):\n",
        "\treturn [randint(2, val_max-1) for _ in range(n)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-N8q1gaBCsy",
        "outputId": "23017cd8-c562-4418-cfed-7357c15e4e16"
      },
      "outputs": [],
      "source": [
        "generate_sequence(k_In_Seq_Length, k_Vocab_Size_In)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01pWZiHjAJFV"
      },
      "outputs": [],
      "source": [
        "# The data we are generating consists in a random sequence of numbers \n",
        "# they could represent encoded letters, words, sentences or anything you could think of.\n",
        "# The target is built using the first elements of the input in reversed order. \n",
        "# We add a special token at the beginning of every target sequence for teacher.\n",
        "# Since words are represented by number and since the translated sentence use the same number k_Vocab_Size_In and k_Vocab_Size_Out are the same  \n",
        "\n",
        "def create_Input_and_Target(n_in, n_out, voc_size, how_many, printing = False):\n",
        "  \n",
        "  X = list()\n",
        "  y  = list()\n",
        "  \n",
        "  for _ in range(how_many):\n",
        "    # generate source sequence\n",
        "    source = generate_sequence(n_in, voc_size)\n",
        "    source_pad = source\n",
        "    if printing:\n",
        "      print(\"source : \", source_pad)\n",
        "    \n",
        "    # add <start> token (0) at the beginning of each sequence\n",
        "    target = source[:n_out]\n",
        "    target.reverse()\n",
        "    target = [0] + target\n",
        "    if printing:\n",
        "      print(\"target : \", target)\n",
        "\n",
        "    X.append(source_pad)\n",
        "    y.append(target)\n",
        "  \n",
        "  return array(X), array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWkizklpBJpz",
        "outputId": "01993fa9-8560-4df0-d688-b2707b2dacae"
      },
      "outputs": [],
      "source": [
        "# Testing purpose\n",
        "# Generate 2 source and 2 target\n",
        "# Sources are \"strings\" of length k_In_Seq_Length words\n",
        "# Targets are \"strings\" of length k_Out_Seq_Length words\n",
        "input, target =  create_Input_and_Target(k_In_Seq_Length, k_Out_Seq_Length, k_Vocab_Size_In, 2, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeOFZcCeFFGj"
      },
      "outputs": [],
      "source": [
        "# Create the training data and validation dataset\n",
        "\n",
        "X_train, y_train = create_Input_and_Target(k_In_Seq_Length, k_Out_Seq_Length, k_Vocab_Size_In, how_many = k_Train_Samples)\n",
        "X_val,   y_val   = create_Input_and_Target(k_In_Seq_Length, k_Out_Seq_Length, k_Vocab_Size_In, how_many = k_Val_Samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSkcX2CUQoHb"
      },
      "outputs": [],
      "source": [
        "# Transform the train sets into batches \n",
        "train_batch   = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(k_Batch_Size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VdBoFXRzYrn"
      },
      "source": [
        "## Create the encoder decoder with attention\n",
        "\n",
        "![bahdanau](https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Deep+Learning/attention/Attention-encoder-decoder.drawio.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPzJHFiSFaeo"
      },
      "source": [
        "### Create encoder model\n",
        "\n",
        "* The goal of the encoder is to create a representation of the input data\n",
        "* This repreentation extract information from the input data which will then be interpreted by the decoder model\n",
        "* The encoder receives sequence inputs \n",
        "* It will output sequences with a given depth of representation (we called that dimension channels before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kquiEvuTHfYw"
      },
      "outputs": [],
      "source": [
        "# Hier on faisait \n",
        "# encoder = tf.keras.Model(inputs = encoder_input, outputs = encoder_output)\n",
        "# plot_model(encoder)\n",
        "# Là on peut plus\n",
        "\n",
        "class encoder_factory(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self, in_vocab_size, embed_dim, n_units):\n",
        "    super().__init__()\n",
        "    self.n_units = n_units\n",
        "    # WE layer\n",
        "    self.embed = tf.keras.layers.Embedding(input_dim = in_vocab_size, output_dim = embed_dim)\n",
        "    # GRU layer\n",
        "    self.gru = tf.keras.layers.GRU(units = n_units, return_sequences = True, return_state = True)\n",
        "  \n",
        "  \n",
        "  def __call__(self, input_batch):\n",
        "    # each output is saved as a class attribute \n",
        "    # doing so we can access them to control the shapes throughout the demo\n",
        "    self.embed_out               = self.embed(input_batch)\n",
        "    self.gru_out, self.gru_state = self.gru(self.embed_out)    \n",
        "    return self.gru_out, self.gru_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpIHpEbDxzHz"
      },
      "outputs": [],
      "source": [
        "# On fait un test \n",
        "encoder = encoder_factory(k_Vocab_Size_In, k_N_Word_Embed, k_N_GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We already generated X_train (size k_Train_Samples)\n",
        "# Realise we can't pass X[train] to the ecoder which expect a tensor \n",
        "print(X_train[0])\n",
        "print(X_train.shape)\n",
        "print(tf.expand_dims(X_train[0],0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkgQs5mXJmHO"
      },
      "outputs": [],
      "source": [
        "# On passe X après l'avoir transfromé en tenseur\n",
        "encoder_output, encoder_state = encoder(tf.expand_dims(X_train[0],0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Y4AA5U61cT",
        "outputId": "dd81a47b-64d1-434c-be72-eb2810c77748"
      },
      "outputs": [],
      "source": [
        "# Remember : k_n_seq = 10 & k_n_gru = 32\n",
        "# The first output as a shape of (1,10,32) which is normal because we applied the encoder to 1 input sequence of 10 elements \n",
        "# (we chose return_sequences = True for the gru layer) and 32 channels since we have 32 units on the gru layer.\n",
        "\n",
        "encoder_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrrjGUr263yS",
        "outputId": "9e5bf764-13ae-44a3-a3b1-cfccfd2f0437"
      },
      "outputs": [],
      "source": [
        "# The second output is the gru state which has shape (1,32) for one input sequence and 32 units on the gru layer.\n",
        "encoder_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9sEGN-Ly0Lb"
      },
      "source": [
        "### Create the Attention layer\n",
        "\n",
        "![bahdanau](https://full-stack-bigdata-datasets.s3.eu-west-3.amazonaws.com/Deep+Learning/attention/Attention-encoder-decoder.drawio.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb-bJzRXz49d"
      },
      "outputs": [],
      "source": [
        "class Bahdanau_attention_factory(tf.keras.layers.Layer):\n",
        "  def __init__(self, attention_units):\n",
        "    super().__init__()\n",
        "\n",
        "    # The attention layer contains three dense layers\n",
        "    self.W1 = tf.keras.layers.Dense(units=attention_units)\n",
        "    self.W2 = tf.keras.layers.Dense(units=attention_units)\n",
        "    self.V  = tf.keras.layers.Dense(units=1)                 # ! obligatoirement 1 seul neurone\n",
        "\n",
        "  def __call__(self, enc_out, state):\n",
        "    # enc_out represents the encoder output which will be used to create the attention weights and then used to create the context vector once we\n",
        "    # apply the attention weights \n",
        "    # the state will be a hidden state from a recurrent unit coming either from the encoder at first, and from the decoder as we make further predictions\n",
        "    self.W1_out = self.W1(enc_out)                                              # shape (1, 10, attention_units)\n",
        "\n",
        "    # we are going to sum the outputs from W1 and W2, though the shapes are incompatible\n",
        "    # the enc_out is (batch_size, 10, 32) -> W1 -> (batch_size,10,attention_units)\n",
        "    # the state is   (batch_size, 32)     -> W2 -> (batch_size, attention_units)\n",
        "    # thus we need to artificially add a dimension to the state along axis 1\n",
        "    self.state  = tf.expand_dims(state, axis = 1) \n",
        "    self.W2_out = self.W2(self.state)                                           # shape (batch_size, 1, attention_units)\n",
        "    self.sum        = self.W1_out + self.W2_out                                 # shape (batch_size, 10, attention_units)\n",
        "    \n",
        "    # tanh because we have positive and negatives values and we want to scale beetween -1 and 1                              \n",
        "    self.sum_scale  = tf.nn.tanh(self.sum)                                      # shape (batch_size, 10, attention_units)\n",
        "\n",
        "    self.score = self.V(self.sum_scale)                                         # shape (batch_size, 10, 1)\n",
        "    self.attention_weights = tf.nn.softmax(self.score, axis=1)                  # shape (batch_size, 10, 1)\n",
        "    self.weighted_enc_out = enc_out * self.attention_weights                    # shape (batch_size, 10, 32)\n",
        "    self.context_vector = tf.reduce_sum(self.weighted_enc_out, axis=1)          # shape (batch_size, 32)\n",
        "\n",
        "    return self.context_vector, self.attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U49YxP_s6v11"
      },
      "outputs": [],
      "source": [
        "attention_layer = Bahdanau_attention_factory(k_N_W1_W2)                         # k_N_W1_W2 neurones dans les couches denses W1 et W2\n",
        "attention_layer(encoder_output, encoder_state)                                  # on regarde ce qui se passe quand on lui passe \n",
        "\n",
        "# the first output is context vector\n",
        "# the second is weight attention vectore (32 de long because 32 in the GRU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Bd5XPCPrTJ"
      },
      "source": [
        "### Create decoder\n",
        "\n",
        "* The decoder use the encoder output and the previous target element to predict the next target element\n",
        "* Its output is a sequence with as many elements as the target \n",
        "* This is where the padded target comes in\n",
        "* It will serve as input and must have a number of channels equals to the number of possible values for target elements\n",
        "* \n",
        "* Two versions of the same model (with the same weights) have to be prepared\n",
        "    * One for training\n",
        "    * One for inference "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./decoder.png\"  />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zk0-USwT8k1o"
      },
      "outputs": [],
      "source": [
        "class decoder_factory(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self, target_vocab_size, embed_dim, n_units):\n",
        "    super().__init__()\n",
        "    # The decoder contains \n",
        "    #     WE layer used with teacher forcing\n",
        "    #     GRU layer\n",
        "    #     Dense layer to make the predictions\n",
        "    #     Attention layer\n",
        "\n",
        "    # ! NOTE that the Embedding layer has an input size of target vocab size (10 000 wordds foar example)\n",
        "    # This is because in inference mode, at the end of the first iteration, pred(X) is connected to the word embedding layer\n",
        "    # pred(x) is a vector of size vocab_size whose most values are null exept the one which is the index of the translated word in the target vocabulary\n",
        "    self.embed = tf.keras.layers.Embedding(input_dim=target_vocab_size, output_dim=embed_dim)\n",
        "    \n",
        "    # ! return_state=True is important since we want to connect state to W2 at the next iteration \n",
        "    self.gru = tf.keras.layers.GRU(units=n_units, return_sequences=True, return_state=True)     \n",
        "    self.pred = tf.keras.layers.Dense(units = target_vocab_size, activation=\"softmax\")\n",
        "    self.attention = Bahdanau_attention_factory(attention_units=n_units)\n",
        "\n",
        "  def __call__(self, dec_in, enc_out, state):\n",
        "    # The Attention layer provides context_vector and  attention weights\n",
        "    self.context_vector, self.attention_weights = self.attention(enc_out,state)\n",
        "\n",
        "    # the decoder ingest one sequence element from the teacher forcing whose shape is (bacth_size, 1)\n",
        "    self.embed_out = self.embed(dec_in)                                                        # shape (batch_size,1,embed_dim)\n",
        "\n",
        "    # concatenate the embedding output and the context vector\n",
        "    # ! their shapes are incompatible\n",
        "    # embed out is of size      : (batch_size, 1, embed_dim)\n",
        "    # context vector is of size : (batch_size, n_units)                                         n_units is defined in the encoder\n",
        "    # => need to add one dimension along axis 1\n",
        "    self.context_vector_expanded = tf.expand_dims(self.context_vector, axis=1)                  # shape (batch_size, 1, n_units)\n",
        "    self.concat = tf.keras.layers.concatenate([self.embed_out, self.context_vector_expanded])   # shape (bacth_size, 1, embed_dim + n_units)\n",
        "    \n",
        "    self.gru_out, self.gru_state = self.gru(self.concat)                                        # shapes (batch_size, 1, n_units) and (batch_size, n_units)\n",
        "\n",
        "    self.gru_out_reshape = tf.reshape(self.gru_out, shape=(-1, self.gru_out.shape[2]))          # pourquoi un reshape ici ??? On est (1, 1, 32) on passe en (1, 32)\n",
        "                                                                                                # On met en (1,32) pour pouvoir le réutiliser ensuite dans la boucle\n",
        "                                                                                                # où on fait un concatenate avec context vector\n",
        "\n",
        "    self.pred_out = self.pred(self.gru_out_reshape)                                             # shape (batch_size, 1, tar_vocab_size)\n",
        "\n",
        "    return self.pred_out, self.gru_state, self.attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPLQRrgfF49C"
      },
      "outputs": [],
      "source": [
        "# Testing : use the decoder using the encoder output, the encoder state and the first element of the teacher forcing\n",
        "# ! On force target vocab size à la même taille que le vocab size input \n",
        "decoder = decoder_factory(target_vocab_size=k_Vocab_Size_In, embed_dim=k_N_Word_Embed, n_units=k_N_GRU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj54hhGHHHeJ"
      },
      "outputs": [],
      "source": [
        "# the teacher forcing is the first element of the target sequence \n",
        "# use expand_dims twice to feed the decoder with a tensor with right dims\n",
        "decoder_input = tf.expand_dims(tf.expand_dims(y_train[0][0], axis=0), axis=0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XY5Sz4gJVbJ",
        "outputId": "d09e3052-087d-4928-e079-a9e171f18c9f"
      },
      "outputs": [],
      "source": [
        "decoder_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsQTURM5IynC",
        "outputId": "16d875ec-b18c-414d-e882-c38c05ea032f"
      },
      "outputs": [],
      "source": [
        "# The first tensor is pred(X) whose size iz vocab_size\n",
        "# The second tensor is the hidden state of the decoder whowse size is n_units\n",
        "decoder(decoder_input, encoder_output, encoder_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBbC19IPxGju"
      },
      "source": [
        "## Training the encoder decoder model\n",
        "\n",
        "The encoder output is used for each prediction once weighted by the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X6jktGKyIok"
      },
      "outputs": [],
      "source": [
        "optimizer     = tf.keras.optimizers.Adam()\n",
        "\n",
        "# SparseCategoricalCrossentropy because \n",
        "# CrossEntropy since it is multi categorieq\n",
        "# Sparce beacause the label are integers (not one hot encoded)\n",
        "# https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb4UZsQMS8Lg"
      },
      "outputs": [],
      "source": [
        "\n",
        "checkpoint_dir    = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint        = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFSjvz22KxsW"
      },
      "outputs": [],
      "source": [
        "def train_step(inp, targ):\n",
        "  loss = 0\n",
        "\n",
        "  # gradient tape to track the operations happening in the network in order to be able to compute the gradients later\n",
        "  with tf.GradientTape() as tape: \n",
        "     # the input sequence is fed to the encoder to produce the encoder output and the encoder state\n",
        "    enc_output, enc_state = encoder(inp)\n",
        "\n",
        "    # the initial state is the encoder state\n",
        "    dec_state = enc_state \n",
        "\n",
        "    # the first decoder input is the first sequence element of the target batch\n",
        "    # the <start> token for each sequence in the batch. \n",
        "    # This is the teacher forcing\n",
        "    dec_input = tf.expand_dims(targ[:,0], axis=1) \n",
        "\n",
        "    # we loop over the teacher forcing sequence to produce the predictions\n",
        "    # we loop from 1 to targ.shape[1] which is the target sequence length\n",
        "    \n",
        "    # t comme token\n",
        "    # targ c'est un batch de token (dim 32)\n",
        "    # Dans une boucle on regarde tous les indice 0, tous les indices 1...\n",
        "    # t = 2 on regarde en même temps \n",
        "    for t in range(1, targ.shape[1]):                                        # range 1... car on a dejà 0\n",
        "      # passing dec_input, dec_state and enc_output to the decoder\n",
        "      # in order to produce the prediction, the new state, and the attention weights which we will not need explicitely here\n",
        "      pred, dec_state, _ = decoder(dec_input, enc_output, dec_state)\n",
        "\n",
        "      # loss sur le token t du batch targ\n",
        "      # compare the prediction produced by teacher forcing with the next element of the target and increment the loss\n",
        "      loss += loss_function(targ[:, t], pred) \n",
        "\n",
        "      # The new decoder input becomes the next element of the target sequence which we just attempted to predict (teacher forcing)\n",
        "      # a l'itération t change. A la dernière iteration on utilise ...\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)                      \n",
        "\n",
        "  # On est en training\n",
        "  # On vient de faire une forward pass\n",
        "  # faut calculer la loss (qui a été incrémenté à chaque tour de boucle)\n",
        "  # rechercher la variable loss_function\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1])) # we divide the loss by the target\n",
        "  # sequence's length to get the average loss across the sequence\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables # here\n",
        "  # we concatenate the lists of trainable variables for the encoder and the\n",
        "  # decoder\n",
        "\n",
        "  # compute the gradient based on the loss and the trainable variables\n",
        "  gradients = tape.gradient(loss, variables) \n",
        "\n",
        "  # then update the model's  parameters\n",
        "  optimizer.apply_gradients(zip(gradients, variables)) \n",
        "\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_829K5bKQS6z",
        "outputId": "4854c9ea-5789-41fe-c40d-17666dafab32"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for epoch in range(k_Epochs):\n",
        "  start = time.time()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_batch):\n",
        "    batch_loss = train_step(inp, targ)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 10 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  \n",
        "  # saving (checkpoint) the model every epoch\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss))\n",
        "  print('Time taken for 1 epoch {} sec'.format(time.time() - start))\n",
        "\n",
        "  # classic encoder input\n",
        "  enc_input = X_val\n",
        "\n",
        "  # the first decoder input is the special token 0\n",
        "  dec_input = tf.zeros(shape=(len(X_val), 1))\n",
        "\n",
        "  \n",
        "  # compute once and for all the encoder output and the encoder h state and c state\n",
        "  enc_out, enc_state = encoder(enc_input)\n",
        "\n",
        "  # The encoder h state and c state will serve as initial states for the decoder\n",
        "  dec_state = enc_state\n",
        "\n",
        "  # we'll store the predictions in here\n",
        "  pred = []  \n",
        "\n",
        "  # we loop over the expected length of the target, but actually the loop can run\n",
        "  # for as many steps as we wish, which is the advantage of the encoder decoder\n",
        "  # architecture\n",
        "  \n",
        "  # Là on fait une inference sur le val set\n",
        "  # On pointe sur le start et après on boucle\n",
        "  for i in range(y_val.shape[1]-1):\n",
        "    dec_out, dec_state, attention_w = decoder(dec_input, enc_out, dec_state)\n",
        "    # the decoder state is updated and we get the first prediction probability vector\n",
        "    decoded_out = tf.expand_dims(tf.argmax(dec_out, axis=-1), axis=1)\n",
        "\n",
        "    # decode the softmax vector into and index and update the prediction list\n",
        "    pred.append(tf.expand_dims(dec_out, axis = 1)) \n",
        "\n",
        "    # the last pred is used as the new input\n",
        "    dec_input = decoded_out \n",
        "\n",
        "  pred = tf.concat(pred, axis=1).numpy()\n",
        "  print(\"\\n val loss :\", loss_function(y_val[:,1:], pred),\"\\n\") # on peut alors afficher la loss sur le val set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaF316p_02jy"
      },
      "source": [
        "Nice! The training is over, and it looks as though the model performs really well both on train and validation sets!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4C6vHoo1NqW"
      },
      "source": [
        "## Make predictions with the inference model\n",
        "\n",
        "To make predictions on the validation set, we cannot use teacher forcing, the model has to base itself on its own predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgymx0a41s-g",
        "outputId": "ad51b7b1-50d5-4986-e6e4-d97898cc5adb"
      },
      "outputs": [],
      "source": [
        "# le val set fait 5000\n",
        "# on fait pareil qu'avant en fait\n",
        "\n",
        "\n",
        "enc_input = X_val # 5000 seq en anglais\n",
        "#classic encoder input\n",
        "\n",
        "dec_input = tf.zeros(shape=(len(X_val),1))                 # 5000 token start\n",
        "# the first decoder input is the special token 0\n",
        "\n",
        "#initial_state = encoder.state_initializer(len(X_val))\n",
        "\n",
        "enc_out, enc_state = encoder(enc_input)#, initial_state)\n",
        "# we compute once and for all the encoder output and the encoder\n",
        "# h state and c state\n",
        "\n",
        "dec_state = enc_state\n",
        "# The encoder h state and c state will serve as initial states for the\n",
        "# decoder\n",
        "\n",
        "pred = []  # we'll store the predictions in here\n",
        "\n",
        "# we loop over the expected length of the target, but actually the loop can run\n",
        "# for as many steps as we wish, which is the advantage of the encoder decoder\n",
        "# architecture\n",
        "for i in range(y_val.shape[1]-1):\n",
        "  dec_out, dec_state, attention_w = decoder(dec_input, enc_out, dec_state)\n",
        "  # the decoder state is updated and we get the first prediction probability \n",
        "  # vector\n",
        "  decoded_out = tf.expand_dims(tf.argmax(dec_out, axis=-1), axis=1) # argmax pour trouver le mot prdit, on l'enregistre\n",
        "  # we decode the softmax vector into and index\n",
        "  pred.append(decoded_out) # update the prediction list\n",
        "  dec_input = decoded_out # the previous pred will be used as the new input\n",
        "\n",
        "pred = tf.concat(pred, axis=-1).numpy()\n",
        "for i in range(10):\n",
        "  print(\"pred:\", pred[i,:].tolist())\n",
        "  print(\"true:\", y_val[i,:].tolist()[1:])\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN9kGOmC3E57"
      },
      "source": [
        "The results do not look so bad, almost perfect actually! This is a clear improvement from the encoder decoder! Attention must be really powerful!\n",
        "\n",
        "The fact that the model reuses the encoder output at each step with different weights is helping the model achieve better predictions in a shorter amount of time (understand epochs).\n",
        "\n",
        "I hope you found this demonstration useful! Now it is time for you to apply what you have learned to a real world automatic translation problem!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "03-Code_Attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
